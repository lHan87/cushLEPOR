# cushLEPOR
cushLEPOR submission to WMT2021 Metric Shared Task

place holder for the data to be shared.

cushLEPOR(LM): tuned version using language model LaBSE (Language-agnostic BERT Sentence Embedding) by Feng et al (2020).

cushLEPOR(pSQM): tuned version using human professional annotated score labels (Scalar Quality Metric ) using WMT20 data by Freitag et al (2021).

- readily tuned cushLEPOR for en-de: the parameter setting;
- readily tuned cushLEPOR for zh-en: the parameter setting;
- relying package: hLEPOR (https://pypi.org/project/hLepor/)
- dependent package: WMT20-MQM/pSQM ( the original link https://github.com/google/wmt-mqm-human-evaluation)
- dependent LM: LaBSE (the original LaBSE link: https://github.com/bojone/labse)

- reference paper (or bibtex format): 

    Gleb Erofeev, Irina Sorokina, Aaron Li-Feng Han, and Serge Gladkoff. 2021. cushlepor uses labse distilled knowledge to improve correlation with human translations. In Proceedings for the MT summit - User Track (In Press), online. Association for Computa- tional Linguistics & AMTA.

    Also ''cushLEPOR: Customised hLEPOR Using LABSE Distilled Knowledge Model to Improve Agreement with Human Judgements".  In WMT21 metrics task.
